{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDA Text Analysis with Sentence Transformers\n",
    "\n",
    "## Overview\n",
    "This notebook implements semantic analysis of Chinese Management Discussion and Analysis (MDA) documents using Sentence Transformers. The code processes Chinese text from MDA reports, converts sentences into semantic vectors, and enables various text analysis tasks such as similarity comparison, clustering, and semantic search.\n",
    "\n",
    "## Purpose\n",
    "- Convert Chinese MDA text into semantic vector representations\n",
    "- Enable semantic similarity analysis between sentences\n",
    "- Support advanced text analysis tasks like clustering and topic modeling\n",
    "- Facilitate semantic search across MDA documents\n",
    "\n",
    "## Key Features\n",
    "- Multilingual sentence embedding using SentenceTransformer\n",
    "- Support for Chinese text processing\n",
    "- High-dimensional semantic vector generation\n",
    "- Efficient vector operations for similarity analysis\n",
    "- Progress tracking during encoding process\n",
    "\n",
    "## Technical Details\n",
    "The implementation includes:\n",
    "1. Text preprocessing:\n",
    "   - Sentence segmentation\n",
    "   - Text cleaning\n",
    "2. Sentence embedding:\n",
    "   - Uses multilingual-MiniLM-L12-v2 model\n",
    "   - Generates 384-dimensional vectors\n",
    "   - Preserves semantic meaning in vector space\n",
    "3. Vector analysis capabilities:\n",
    "   - Similarity computation\n",
    "   - Clustering\n",
    "   - Semantic search\n",
    "\n",
    "## Requirements\n",
    "- Python 3.x\n",
    "- sentence-transformers\n",
    "- torch\n",
    "- transformers\n",
    "- huggingface_hub[hf_xet] (recommended for better performance)\n",
    "- numpy\n",
    "- pandas (for data manipulation)\n",
    "\n",
    "## Usage\n",
    "1. Install required packages:\n",
    "   ```bash\n",
    "   pip install sentence-transformers torch transformers huggingface_hub[hf_xet]\n",
    "   ```\n",
    "\n",
    "2. Prepare your Chinese MDA text data\n",
    "3. Run the sentence embedding process\n",
    "4. Use the generated vectors for your analysis\n",
    "\n",
    "## Output\n",
    "The code generates:\n",
    "- Semantic vectors for each sentence (384 dimensions)\n",
    "- Vector representations suitable for:\n",
    "  - Similarity analysis\n",
    "  - Clustering\n",
    "  - Semantic search\n",
    "  - Topic modeling\n",
    "\n",
    "## Interpretation\n",
    "- Each sentence is converted into a 384-dimensional vector\n",
    "- Similar sentences will have similar vector representations\n",
    "- Vector distances can be used to measure semantic similarity\n",
    "- The vectors capture semantic meaning rather than just word overlap\n",
    "- The multilingual model is specifically trained to handle Chinese text\n",
    "\n",
    "## Applications\n",
    "- Finding similar sentences across different MDA reports\n",
    "- Identifying common themes and topics\n",
    "- Semantic search in financial documents\n",
    "- Document clustering and organization\n",
    "- Topic modeling and theme extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences loaded: 7186\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os # Provides a way of using operating system dependent functionality, like reading directories and files.\n",
    "import jieba # A popular Chinese text segmentation (word tokenization) library. Although imported, it's not used in this specific cell.\n",
    "import re # Provides regular expression operations, used here for sentence splitting.\n",
    "from sentence_transformers import SentenceTransformer # Library for generating sentence embeddings. Although imported, it's not used in this specific cell.\n",
    "from sklearn.cluster import KMeans # Library for performing KMeans clustering. Although imported, it's not used in this specific cell.\n",
    "\n",
    "# ---- Step 1: Load and Sentence-Split All .txt Files ----\n",
    "# Define the path to the folder containing the text files.\n",
    "# \"../testMDA\" means go up one directory from the notebook's location and then into the \"testMDA\" folder.\n",
    "folder_path = \"../testMDA\"\n",
    "\n",
    "# Initialize an empty list to store all extracted sentences from all files.\n",
    "all_sentences = []\n",
    "\n",
    "# Initialize an empty list to keep track of the original filename for each sentence.\n",
    "# This helps in mapping sentences back to their source document later if needed.\n",
    "file_sentence_map = []\n",
    "\n",
    "# Define a function to split a given text into individual sentences.\n",
    "def split_sentences(text):\n",
    "    # Use regular expressions to split the text.\n",
    "    # The pattern r'[。！？!?\\.]' looks for Chinese full stop (。), exclamation mark (！),\n",
    "    # question mark (？), English exclamation mark (!), question mark (?), or full stop (.).\n",
    "    # The text will be split at each occurrence of these characters.\n",
    "    sentences = re.split(r'[。！？!?\\.]', text)\n",
    "\n",
    "    # Process the split sentences:\n",
    "    # 1. s.strip() removes leading/trailing whitespace from each potential sentence fragment.\n",
    "    # 2. s.strip() checks if the fragment is not empty after stripping whitespace.\n",
    "    # 3. len(s.strip()) > 8 filters out fragments that are shorter than 8 characters after stripping.\n",
    "    #    This helps to remove incomplete sentences or very short phrases that might result from splitting.\n",
    "    return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 8]\n",
    "\n",
    "# Loop through all files and directories in the specified folder path.\n",
    "for file in os.listdir(folder_path):\n",
    "    # Check if the current item is a file and ends with the \".txt\" extension.\n",
    "    if file.endswith(\".txt\"):\n",
    "        # Construct the full path to the text file.\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        # Open the text file for reading ('r') with UTF-8 encoding to handle various characters, especially Chinese.\n",
    "        # 'with open(...) as f:' ensures the file is automatically closed after the block.\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # Read the entire content of the file.\n",
    "            text = f.read()\n",
    "            # Replace newline characters ('\\n') with spaces (' ').\n",
    "            # This helps in treating paragraphs as continuous text for sentence splitting.\n",
    "            text = text.replace('\\n', ' ')\n",
    "\n",
    "            # Split the read text into sentences using the defined function.\n",
    "            sentences = split_sentences(text)\n",
    "\n",
    "            # Add the extracted sentences from the current file to the main list of all sentences.\n",
    "            all_sentences.extend(sentences)\n",
    "\n",
    "            # Add the filename to the file_sentence_map list for each sentence extracted from this file.\n",
    "            # [file]*len(sentences) creates a list containing the filename repeated 'len(sentences)' times.\n",
    "            file_sentence_map.extend([file]*len(sentences))\n",
    "\n",
    "# After processing all files, print the total number of sentences that were loaded and extracted.\n",
    "print(f\"Total sentences loaded: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading multilingual sentence transformer model...\n",
      "Model loaded successfully!\n",
      "Model loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 225/225 [00:52<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 2: Sentence Embedding ----\n",
    "# This step involves converting the extracted text sentences into numerical representations (vectors or embeddings).\n",
    "# These embeddings capture the semantic meaning of the sentences, allowing for comparisons and clustering.\n",
    "\n",
    "# Print a message indicating that the model loading process is starting.\n",
    "print(\"Loading multilingual sentence transformer model...\")\n",
    "\n",
    "# Use a try-except block to handle potential errors during model loading,\n",
    "# such as network issues or the model not being found locally.\n",
    "try:\n",
    "    # Attempt to load the pre-trained Sentence Transformer model.\n",
    "    # 'paraphrase-multilingual-MiniLM-L12-v2' is a specific model trained to\n",
    "    # produce embeddings for sentences in many different languages (multilingual).\n",
    "    # It's optimized for paraphrase identification and provides relatively fast\n",
    "    # and good quality embeddings.\n",
    "    # While another alternative is model = SentenceTransformer('shibing624/text2vec-base-chinese')\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# If an exception occurs during the initial load attempt (e.g., model not found locally),\n",
    "# catch the exception and print an error message.\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    # Print a message indicating that a download attempt will be made.\n",
    "    print(\"Attempting to download model...\")\n",
    "    # Try another try-except block for the download attempt.\n",
    "    try:\n",
    "        # Attempt to load the model again, this time using the full path\n",
    "        # from the sentence-transformers organization on Hugging Face Models.\n",
    "        # This often forces a download if the model isn't cached locally.\n",
    "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    # If the download attempt also fails, catch the exception.\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download model: {e}\")\n",
    "        # Provide guidance to the user on potential causes (internet connection).\n",
    "        print(\"Please check your internet connection and try again.\")\n",
    "        # Re-raise the exception to stop execution, as the model is required for the next steps.\n",
    "        raise\n",
    "\n",
    "# If the code reaches this point, it means the model was loaded successfully (either from cache or downloaded).\n",
    "# Print a confirmation message.\n",
    "print(\"Model loaded successfully!\")\n",
    "# Remove the get_config_dict() call as it's not available\n",
    "# Print the loaded model object for verification.\n",
    "print(f\"Model loaded: {model}\")\n",
    "\n",
    "# Continue with sentence encoding\n",
    "# Use the loaded model to encode all the sentences collected in the previous step (all_sentences).\n",
    "# The .encode() method converts each sentence string into a numerical vector (embedding).\n",
    "# show_progress_bar=True displays a progress bar during the encoding process, which can take time for many sentences.\n",
    "# The resulting embeddings are stored in the sentence_vecs variable, typically as a NumPy array.\n",
    "sentence_vecs = model.encode(all_sentences, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sentence vectors: (7186, 384)\n",
      "Type of sentence vectors: <class 'numpy.ndarray'>\n",
      "Data type of vectors: float32\n",
      "\n",
      "First 3 sentence vectors (first 5 dimensions):\n",
      "Sentence 1: [-0.19068159  0.24712619 -0.15997165 -0.20050922  0.12583588]\n",
      "Sentence 2: [-0.12212664  0.16387533 -0.25085154 -0.03061001  0.10135089]\n",
      "Sentence 3: [ 0.16982849  0.06446873 -0.09906378 -0.03315345  0.26703802]\n",
      "\n",
      "Vector statistics:\n",
      "Mean value: 0.0004\n",
      "Standard deviation: 0.1837\n",
      "Min value: -0.9992\n",
      "Max value: 1.4429\n",
      "\n",
      "First 3 sentences with their vectors:\n",
      "\n",
      "Sentence 1:\n",
      "Text: 1  总体经营情况 2022  年，党的二十大胜利召开，这是在全党全国各族人民迈上全面建设社会主义现代化国家新征程、向第二个百年奋斗目标进军的关键时刻召开的一次十分重要的大会\n",
      "Vector (first 5 dimensions): [-0.19068159  0.24712619 -0.15997165 -0.20050922  0.12583588]\n",
      "\n",
      "Sentence 2:\n",
      "Text: 大会通过的报告，擘画了全面建成社会主义现代化强国的宏伟蓝图和实践路径，就未来五年党和国家事业发展制定了大政方针、作出了全面部署，为金融业的未来发展指明了方向\n",
      "Vector (first 5 dimensions): [-0.12212664  0.16387533 -0.25085154 -0.03061001  0.10135089]\n",
      "\n",
      "Sentence 3:\n",
      "Text: 本行积极贯彻落实党的二十大精神，不断提升金融服务实体经济的能力，持续加大对居民消费、民营企业、小微企业、制造业、涉农等领域的金融支持力度，积极助力扩大内需，积极践行绿色金融，大力支持乡村振兴，持续强化全面风险管理，全力助推高质量发展，业务发展保持了稳健增长的态势\n",
      "Vector (first 5 dimensions): [ 0.16982849  0.06446873 -0.09906378 -0.03315345  0.26703802]\n"
     ]
    }
   ],
   "source": [
    "# Print basic information about the sentence vectors\n",
    "print(f\"Shape of sentence vectors: {sentence_vecs.shape}\")\n",
    "print(f\"Type of sentence vectors: {type(sentence_vecs)}\")\n",
    "print(f\"Data type of vectors: {sentence_vecs.dtype}\")\n",
    "\n",
    "# Print first few vectors as example\n",
    "print(\"\\nFirst 3 sentence vectors (first 5 dimensions):\")\n",
    "for i in range(3):\n",
    "    print(f\"Sentence {i+1}: {sentence_vecs[i][:5]}\")\n",
    "\n",
    "# Print some statistics\n",
    "print(\"\\nVector statistics:\")\n",
    "print(f\"Mean value: {sentence_vecs.mean():.4f}\")\n",
    "print(f\"Standard deviation: {sentence_vecs.std():.4f}\")\n",
    "print(f\"Min value: {sentence_vecs.min():.4f}\")\n",
    "print(f\"Max value: {sentence_vecs.max():.4f}\")\n",
    "\n",
    "# If you want to see the actual sentences along with their vectors\n",
    "print(\"\\nFirst 3 sentences with their vectors:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nSentence {i+1}:\")\n",
    "    print(f\"Text: {all_sentences[i]}\")\n",
    "    print(f\"Vector (first 5 dimensions): {sentence_vecs[i][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sentences into 10 topics...\n"
     ]
    }
   ],
   "source": [
    "# ---- Step 3: Cluster Sentences (Topic Extraction) ----\n",
    "num_topics = 10  # You may adjust this based on data size/desired granularity\n",
    "\n",
    "print(f\"Clustering sentences into {num_topics} topics...\")\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(sentence_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Top Sentences from Each Topic (cluster) ----\n",
      "\n",
      "[Topic 1]: (768 sentences)\n",
      "- (1-平安银行-2022.txt): 1%，其中，发放贷款和垫款本金总额 33,291\n",
      "- (1-平安银行-2022.txt): 负债总额 48,868\n",
      "- (1-平安银行-2022.txt): 2022 年末，不良贷款率 1\n",
      "- (1-平安银行-2022.txt): 03 个百分点；逾期贷款余额占比 1\n",
      "- (1-平安银行-2022.txt): 05 百分点；逾期 60 天以上贷款偏离度及逾期 90 天以上贷款偏离度分别为 0\n",
      "- (1-平安银行-2022.txt): 081 存放中央银行款项 3715 0\n",
      "- (1-平安银行-2022.txt): 033 存放同业、拆放同业及买入返售金融资产 4795 0\n",
      "- (1-平安银行-2022.txt): 4%) 发放贷款和垫款（含贴现） 188344 0\n",
      "\n",
      "[Topic 2]: (616 sentences)\n",
      "- (1-平安银行-2022.txt): 04；拨备覆盖率 290\n",
      "- (1-平安银行-2022.txt): 0301 存放央行 249879 3715 0\n",
      "- (1-平安银行-2022.txt): 015 同业业务 437604 12415 0\n",
      "- (1-平安银行-2022.txt): 0289 生息资产总计 4738938 228878 0\n",
      "- (1-平安银行-2022.txt): 0293 其中：同业存单 608410 15407 0\n",
      "- (1-平安银行-2022.txt): 028 同业业务及其他 633752 12304 0\n",
      "- (1-平安银行-2022.txt): 0405 净利差 0\n",
      "- (1-平安银行-2022.txt): 0274 净息差 0\n",
      "\n",
      "[Topic 3]: (1247 sentences)\n",
      "- (1-平安银行-2022.txt): 016 3595 0\n",
      "- (1-平安银行-2022.txt): 021 4814 0\n",
      "- (1-平安银行-2022.txt): 823 173736 0\n",
      "- (1-平安银行-2022.txt): 086 10604 0\n",
      "- (1-平安银行-2022.txt): 678 57027 0\n",
      "- (1-平安银行-2022.txt): 059 2853155 173736 0\n",
      "- (1-平安银行-2022.txt): 0283 739031 22264 0\n",
      "- (1-平安银行-2022.txt): 0149 240415 3595 0\n",
      "\n",
      "[Topic 4]: (527 sentences)\n",
      "- (1-平安银行-2022.txt): 本行建立了有针对性的风险管理和内部控制体系，有效识别、计量、监测、报告和控制衍生品投资相关风险\n",
      "- (1-平安银行-2022.txt): 本行设置了专门的风险管理机构，建立了有针对性的风险管理体系，有效管理衍生品投资业务风险\n",
      "- (1-平安银行-2022.txt): 在权益体系建设上，本行积极整合平安集团内外部专业资源，重点打造高端医养、子女教育、慈善规划等多个王牌权益，满足稀缺性非金融服务需求，向客户提供全生命周期陪伴式服务\n",
      "- (1-平安银行-2022.txt): “平安公益平台”积极响应抗震救灾号召，联合中华社会救助基金会、“壹基金”上线驰援泸定灾区项目，提供救援物资支持\n",
      "- (1-平安银行-2022.txt): 通过全面贯彻以客户为中心的经营理 念，深入推进“智能化银行 3\n",
      "- (1-平安银行-2022.txt): 0”建设；同时，持续推动综合经营、协同发展，强化全渠道综合化获客及全场景智能化经营\n",
      "- (1-平安银行-2022.txt): 二是以客户需求为中心，升级代发客群数字化经营体系，全方位赋能客群精细化经营；2022  年末，本行代发及批量业务客户带来的 AUM 余额 6,599\n",
      "- (1-平安银行-2022.txt): 口袋银行升级方面，本行升级平安口袋银行 APP 智能化平台布局，打造数字化经营的主门户\n",
      "\n",
      "[Topic 5]: (649 sentences)\n",
      "- (1-平安银行-2022.txt): 深化资产负债经营，净息差稳中略降\n",
      "- (1-平安银行-2022.txt): 强化金融风险防控，资产质量保持平稳\n",
      "- (1-平安银行-2022.txt): 86 个百分点，风险抵补能力保持较好水平\n",
      "- (1-平安银行-2022.txt): 2  财务报表分析 3\n",
      "- (1-平安银行-2022.txt): 1  利润表项目分析 （1）营业收入构成及变动情况 2022 年，本集团实现营业收入 1,798\n",
      "- (1-平安银行-2022.txt): 8%；另一方面，本行积极重塑资产负债经营，负债端：主动优化负债结构，压降负债成本；资产端：受市场利率下行、持续让利实体经济等因素影响，资产收益率下降，净息差有所收窄\n",
      "- (1-平安银行-2022.txt): 主要资产、负债项目的日均余额以及平均收益率或平均成本率的情况 （货币单位：人民币百万元） 项  目 2022 年 1-12 月 2021 年 1-12 月 日均余额 利息收入/支出 平均收益/成本率 日均余额 利息收入/支出 平均收益/成本...\n",
      "- (1-平安银行-2022.txt): 90%，同比 2021 年下降 12 个基点，主要是贷款市场报价利率（LPR）下降带动贷款利率下 行，同时本行主动下调贷款利率，落实金融支持实体经济政策，企业贷款收益率随之下降；个人贷 款平均收益率 7\n",
      "\n",
      "[Topic 6]: (439 sentences)\n",
      "- (1-平安银行-2022.txt): 118 （3）股东权益变动情况 2022 年末，本集团股东权益 4,346\n",
      "- (10-美丽生态-2022.txt): 8007 分地区 华东 99968143\n",
      "- (10-美丽生态-2022.txt): 3064 华中 0\n",
      "- (10-美丽生态-2022.txt): 0635 分地区 华东 99968143\n",
      "- (10-美丽生态-2022.txt): 00 专业分包 2018 年08 月 08日 1095 日历天 0\n",
      "- (10-美丽生态-2022.txt): 98 深圳市东部过境高速公路一期 178244098\n",
      "- (10-美丽生态-2022.txt): 1423 3 中铁十七局集团城市建设有限公司 70553677\n",
      "- (10-美丽生态-2022.txt): 1151 4 云南省建设投资控股集团有限公司 68233628\n",
      "\n",
      "[Topic 7]: (869 sentences)\n",
      "- (1-平安银行-2022.txt): 2022 年，本集团实现营业收入 1,798\n",
      "- (1-平安银行-2022.txt): 95 亿元，同比增长 6\n",
      "- (1-平安银行-2022.txt): 2%；实现净利润 455\n",
      "- (1-平安银行-2022.txt): 16 亿元，同比增长 25\n",
      "- (1-平安银行-2022.txt): 3%；加权平均净资产收益率（ROE）为 12\n",
      "- (1-平安银行-2022.txt): 36%，同比上升 1\n",
      "- (1-平安银行-2022.txt): 14 亿元，较上年末增长 8\n",
      "- (1-平安银行-2022.txt): 61 亿元，较上年末增长 8\n",
      "\n",
      "[Topic 8]: (726 sentences)\n",
      "- (1-平安银行-2022.txt): 1  总体经营情况 2022  年，党的二十大胜利召开，这是在全党全国各族人民迈上全面建设社会主义现代化国家新征程、向第二个百年奋斗目标进军的关键时刻召开的一次十分重要的大会\n",
      "- (1-平安银行-2022.txt): 大会通过的报告，擘画了全面建成社会主义现代化强国的宏伟蓝图和实践路径，就未来五年党和国家事业发展制定了大政方针、作出了全面部署，为金融业的未来发展指明了方向\n",
      "- (1-平安银行-2022.txt): 2022 年末，本集团资产总额 53,215\n",
      "- (1-平安银行-2022.txt): 2022 年，本集团净息差 2\n",
      "- (1-平安银行-2022.txt): 09%，同比 2021 年上升 5 个基点\n",
      "- (1-平安银行-2022.txt): 2022 年末，本集团各级资本充足率均满足监管达 标要求，其中核心一级资本充足率 8\n",
      "- (1-平安银行-2022.txt): 一方面，本行持续加大对实体经济的信贷投放力度，2022 年发放贷款和垫款（含贴现）日均余额 31,906\n",
      "- (1-平安银行-2022.txt): 33%，同比 2021 年上升 3 个基点\n",
      "\n",
      "[Topic 9]: (516 sentences)\n",
      "- (1-平安银行-2022.txt): 723 120336 0\n",
      "- (1-平安银行-2022.txt): 14 31391 0\n",
      "- (1-平安银行-2022.txt): 039 3664 0\n",
      "- (1-平安银行-2022.txt): 197 21905 0\n",
      "- (1-平安银行-2022.txt): 277 49047 0\n",
      "- (1-平安银行-2022.txt): 168 33062 0\n",
      "- (1-平安银行-2022.txt): 109 15985 0\n",
      "- (1-平安银行-2022.txt): 005 其他 6882 7131 (3\n",
      "\n",
      "[Topic 10]: (829 sentences)\n",
      "- (1-平安银行-2022.txt): 本行积极贯彻落实党的二十大精神，不断提升金融服务实体经济的能力，持续加大对居民消费、民营企业、小微企业、制造业、涉农等领域的金融支持力度，积极助力扩大内需，积极践行绿色金融，大力支持乡村振兴，持续强化全面风险管理，全力助推高质量发展，业务发...\n",
      "- (1-平安银行-2022.txt): 营收保持稳健增长，盈利能力持续提升\n",
      "- (1-平安银行-2022.txt): 业务规模稳步增长，积极支持实体经济\n",
      "- (1-平安银行-2022.txt): 7%；本行持续加大实体经济支持力度，优化信贷结构，普惠、制造业、涉农、绿色金融等领域贷款实现较好增长\n",
      "- (1-平安银行-2022.txt): 本行积极重塑资产负债经营，负债端主动优化结构，把握市场趋势择机吸收低成本资金；资产端积极应对市场变化，持续加大支持实体经济力度\n",
      "- (1-平安银行-2022.txt): 本行积极应对宏观经济环境变化，加强资产质量管控，推动风险防范和化解\n",
      "- (1-平安银行-2022.txt): 践行精细化管理，核心一级资本充足率提升\n",
      "- (1-平安银行-2022.txt): 0209 本行持续推动对公、零售业务转型，重塑资产负债结构，做好量价平衡\n",
      "\n",
      "Done. Review the topics above for semantic themes.\n"
     ]
    }
   ],
   "source": [
    "# Import Counter from collections (although imported, it's not used in this specific snippet)\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize a list of lists to hold sentences for each topic (cluster).\n",
    "# The number of inner lists is equal to the number of topics (clusters) determined earlier (num_topics).\n",
    "# Each inner list will store tuples of (sentence, original_filename) belonging to that topic.\n",
    "topic_sentences = [[] for _ in range(num_topics)]\n",
    "\n",
    "# Iterate through the assigned cluster labels and the original sentences.\n",
    "# 'labels' is assumed to be a list or array containing the cluster index for each sentence in 'all_sentences'.\n",
    "# 'enumerate(labels)' provides both the index (idx) and the label (cluster ID) for each sentence.\n",
    "for idx, label in enumerate(labels):\n",
    "    # Append the tuple (sentence, filename) to the list corresponding to the sentence's assigned cluster label.\n",
    "    # all_sentences[idx] gets the original sentence string.\n",
    "    # file_sentence_map[idx] gets the original filename for that sentence.\n",
    "    topic_sentences[label].append((all_sentences[idx], file_sentence_map[idx]))\n",
    "\n",
    "# Print a header indicating the start of the topic sentence display.\n",
    "print(\"\\n---- Top Sentences from Each Topic (cluster) ----\")\n",
    "\n",
    "# Iterate through each topic (cluster) and its associated sentences.\n",
    "# 'enumerate(topic_sentences)' provides the topic index (topic_idx) and the list of sentences (sents) for that topic.\n",
    "for topic_idx, sents in enumerate(topic_sentences):\n",
    "    # Print the topic number (using 1-based indexing for readability) and the total count of sentences in that topic.\n",
    "    print(f\"\\n[Topic {topic_idx+1}]: ({len(sents)} sentences)\")\n",
    "\n",
    "    # Initialize a set to keep track of sentences already shown for this topic.\n",
    "    # This helps prevent showing the exact same sentence from the exact same file multiple times if duplicates exist (unlikely with the tuple key).\n",
    "    shown = set()\n",
    "    # Initialize a counter for the number of unique sentences shown for this topic.\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through the sentences within the current topic.\n",
    "    # Each item 'sent, file' is a tuple containing the sentence string and its original filename.\n",
    "    for sent, file in sents:\n",
    "        # Create a unique key for the sentence based on its content and source file.\n",
    "        key = (sent, file)\n",
    "        # Check if this sentence (from this file) has already been shown for this topic.\n",
    "        if key not in shown:\n",
    "            # If not shown, print the sentence.\n",
    "            # Format: \"- (filename): sentence_text\"\n",
    "            # sent[:120] takes the first 120 characters of the sentence.\n",
    "            # {'...' if len(sent) > 120 else ''} adds \"...\" if the original sentence was longer than 120 characters, indicating truncation.\n",
    "            print(f\"- ({file}): {sent[:120]}{'...' if len(sent) > 120 else ''}\")\n",
    "            # Add the key to the 'shown' set to mark it as displayed.\n",
    "            shown.add(key)\n",
    "            # Increment the count of shown sentences for this topic.\n",
    "            count += 1\n",
    "            # If 8 unique sentences have been shown for this topic, break out of the inner loop\n",
    "            # to limit the output per topic.\n",
    "            if count >= 8:\n",
    "                break\n",
    "\n",
    "# Print a final message guiding the user on how to interpret the output.\n",
    "# The goal is to look at the sample sentences within each topic to identify common themes or subjects.\n",
    "print(\"\\nDone. Review the topics above for semantic themes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
