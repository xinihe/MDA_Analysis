{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CorEx Topic Modeling for Chinese MDA Analysis\n",
    "\n",
    "## Overview\n",
    "It implements CorEx (Correlation Explanation) topic modeling to analyze Chinese Management Discussion and Analysis (MDA) documents. The code processes Chinese text from MDA reports, identifies key topics, and extracts meaningful themes that appear across multiple documents.\n",
    "\n",
    "## Limitations\n",
    "- **Word-Level Analysis**: The current implementation operates at the word level, which may not capture the full semantic meaning of sentences and phrases.\n",
    "- **Context Loss**: By focusing on individual words, the model might miss important contextual relationships between words.\n",
    "- **Semantic Understanding**: Word-level topic modeling may not fully understand the nuanced meaning of financial and business concepts.\n",
    "\n",
    "## Recommended Enhancement\n",
    "For more accurate and meaningful analysis, consider using semantic-level approaches such as:\n",
    "- Sentence Transformers for semantic embedding\n",
    "- BERT-based models for contextual understanding\n",
    "- Semantic similarity analysis for theme identification\n",
    "\n",
    "These semantic approaches can better capture:\n",
    "- The true meaning of financial statements\n",
    "- Contextual relationships between concepts\n",
    "- Nuanced business discussions\n",
    "- Complex financial relationships\n",
    "\n",
    "## Technical Details\n",
    "The implementation includes:\n",
    "1. Text preprocessing:\n",
    "   - Chinese word segmentation\n",
    "   - Stop word removal\n",
    "   - Text cleaning\n",
    "2. Document vectorization\n",
    "3. Topic modeling with CorEx\n",
    "4. Topic extraction and visualization\n",
    "\n",
    "## Requirements\n",
    "- Python 3.x\n",
    "- jieba (Chinese text segmentation)\n",
    "- scikit-learn\n",
    "- corextopic\n",
    "- pandas\n",
    "- numpy\n",
    "\n",
    "## Usage\n",
    "1. Place MDA text files in the specified input directory\n",
    "2. Run the notebook cells sequentially\n",
    "3. Review the extracted topics\n",
    "4. Check topics_output.txt for saved results\n",
    "\n",
    "## Output\n",
    "The code generates:\n",
    "- Extracted topics with associated keywords\n",
    "- Topic-word relationships\n",
    "- Saved topic results in text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17 documents.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os  # For file and directory operations\n",
    "import jieba  # Chinese text segmentation library\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # For text vectorization\n",
    "from corextopic import corextopic as ct  # For topic modeling\n",
    "import re\n",
    "\n",
    "# Step 1: Load all .txt files from 'MDA' folder\n",
    "folder_path = \"../testMDA\"  # Path to the folder containing MDA text files\n",
    "doc_names = []  # List to store document filenames\n",
    "raw_docs = []   # List to store raw document contents\n",
    "\n",
    "# Iterate through all files in the specified folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".txt\"):  # Only process .txt files\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().replace(\"\\n\", \"\")  # Remove line breaks for cleaner text\n",
    "            raw_docs.append(text)\n",
    "            doc_names.append(file)\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} documents.\")\n",
    "\n",
    "# Step 2: Chinese segmentation with jieba\n",
    "# jieba.lcut() splits Chinese text into individual words\n",
    "# The result is joined with spaces to create space-separated words\n",
    "segmented_docs = [\" \".join(jieba.lcut(doc)) for doc in raw_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove Chinese stop words\n",
    "# Define common Chinese stop words that don't carry significant meaning\n",
    "stopwords = set([\"的\", \"和\", \"在\", \"是\", \"了\", \"与\", \"也\", \"或\", \"对\", \"有\", \"为\", \"其他\",\"情况\",\n",
    "                 \"就\", \"都\", \"而\", \"及\", \"与\", \"以\", \"到\", \"一个\", \"我们\", \"你们\",\"公司\",\"主要\",\"个人\"])\n",
    "\n",
    "# Function to remove stop words from a document\n",
    "def remove_stopwords(doc):\n",
    "    # Split document into words, filter out stop words, and rejoin with spaces\n",
    "    return \" \".join([word for word in doc.split() if word not in stopwords])\n",
    "\n",
    "chinese_digits = set(\"零一二三四五六七八九十百千万亿两〇\")\n",
    "unit_suffixes = [\n",
    "    \"元\", \"万元\", \"亿元\", \"平方米\", \"公里\", \"千米\", \"公斤\", \"个\", \"户\", \"家\", \"次\", \"年\", \"月\"\n",
    "]\n",
    "\n",
    "def contains_chinese_digit(word):\n",
    "    return any(ch in word for ch in chinese_digits)\n",
    "\n",
    "def contains_arabic_digit(word):\n",
    "    return bool(re.search(r\"\\d\", word))\n",
    "\n",
    "def is_quantifier_token(word):\n",
    "    return any(word.endswith(suffix) for suffix in unit_suffixes)\n",
    "\n",
    "def remove_numbers_and_quantifiers(doc):\n",
    "    return \" \".join([\n",
    "        word for word in doc.split()\n",
    "        if not (contains_arabic_digit(word) or contains_chinese_digit(word) or is_quantifier_token(word))\n",
    "    ])\n",
    "\n",
    "# Usage after segmentation and stopword removal:\n",
    "clean_docs = [remove_stopwords(doc) for doc in segmented_docs]\n",
    "clean_docs = [remove_numbers_and_quantifiers(doc) for doc in clean_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Vectorization\n",
    "# Set the maximum number of features (words) to consider in the vocabulary\n",
    "# A higher number captures more words but increases computation time\n",
    "max_features = 1000  # adjust between 500-2000\n",
    "\n",
    "# Initialize the CountVectorizer with specified parameters\n",
    "# This will convert text documents into a matrix of token counts\n",
    "vectorizer = CountVectorizer(max_features=max_features, ngram_range=(1, 3))\n",
    "\n",
    "# Transform the cleaned documents into a document-term matrix\n",
    "# Each row represents a document, each column represents a word\n",
    "# The values are the frequency of each word in each document\n",
    "doc_word = vectorizer.fit_transform(clean_docs)\n",
    "\n",
    "# Get the list of words (features) that were used in the vectorization\n",
    "# These are the words that will be used for topic modeling\n",
    "words = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Some words never appear (or always appear)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<corextopic.corextopic.Corex at 0x1f2c00e0440>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Topic modeling with CorEx\n",
    "# Set the number of topics to extract\n",
    "# More topics can capture finer-grained themes but may be harder to interpret\n",
    "n_hidden = 8  # Recommended: 5-15\n",
    "\n",
    "# Initialize the CorEx topic model with specified parameters\n",
    "# seed=42 ensures reproducibility of results\n",
    "corex_model = ct.Corex(n_hidden=n_hidden, words=words, seed=42)\n",
    "\n",
    "# Fit the model to the document-term matrix\n",
    "# This will learn the topics from the documents\n",
    "corex_model.fit(doc_word, words=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: 主营业务 分析, 主营业务 数据, 项目 同比 增减, 从事, 分类 项目\n",
      "\n",
      "Topic #2: 产业 发展, 自身, 订单, 物业, 物业管理\n",
      "\n",
      "Topic #3: 能源, 可以, 检测, 动力, 变动 比例 研发\n",
      "\n",
      "Topic #4: 文化, 数字, 年底, 效益, 各类\n",
      "\n",
      "Topic #5: 供应, 股份, 获得, 用于, 增长点\n",
      "\n",
      "Topic #6: 我国, 同比 下降, 存量, 快速, 融资\n",
      "\n",
      "Topic #7: 培育, 收购, 生态, 规范, 构建\n",
      "\n",
      "Topic #8: 较大, 补充, 投资者, 业绩, 拥有\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Display topics\n",
    "# Get the top 10 words for each topic\n",
    "# Each topic is represented by its most characteristic words\n",
    "topics = corex_model.get_topics(n_words=5)\n",
    "\n",
    "# Print each topic and its associated words\n",
    "for idx, topic in enumerate(topics):\n",
    "    words_in_topic = [word for word, *score in topic]  # Extract just the words # mis contains mutual information scores\n",
    "    print(f\"Topic #{idx+1}: {', '.join(words_in_topic)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save topics and associated words to file\n",
    "# This creates a permanent record of the discovered topics\n",
    "# with open(\"topics_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for idx, topic in enumerate(topics):\n",
    "#         words_in_topic = [word for word, *score in topic]\n",
    "#         f.write(f\"Topic #{idx+1}: {', '.join(words_in_topic)}\\n\")\n",
    "        \n",
    "# print(\"Topics extraction completed and saved to topics_output.txt.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
