{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e699357a",
   "metadata": {},
   "source": [
    "股吧评论数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##将获取到的股吧评论数据进行切分，确保每行每列表示一个含义\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 设置文件夹路径\n",
    "folder_path = 'E:/股吧爬虫/股吧爬虫/古巴爬虫5'  # 替换为你的文件夹路径\n",
    "\n",
    "# 创建保存切分后数据的文件夹\n",
    "output_folder = 'E:/股吧爬虫/股吧爬虫/古巴爬虫1/切分'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 遍历文件夹中的每个CSV文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # 构造完整的文件路径\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path, header=None, names=['Data'],engine='python',encoding='utf-8')\n",
    "\n",
    "        # 切分数据\n",
    "        df_split = df['Data'].str.split('\\t', expand=True)\n",
    "\n",
    "        # 构造保存Excel文件的路径\n",
    "        output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.xlsx\")\n",
    "\n",
    "        # 保存切分后的数据为Excel文件\n",
    "        #df_split.to_excel(output_file_path, index=False, encoding='utf-8')\n",
    "        df_split.to_excel(output_file_path, index=False)\n",
    "\n",
    "        print(f\"Processed {filename} and saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e1bda45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已解压到： E:\\股吧爬虫\\股吧爬虫\n",
      "目录列表： ['╣╔░╔┼└│µ']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = r'E:\\股吧爬虫\\股吧爬虫.zip'\n",
    "extract_to = r'E:\\股吧爬虫\\股吧爬虫'  # 解压后存放的目标目录\n",
    "\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    z.extractall(extract_to)\n",
    "\n",
    "print(\"已解压到：\", extract_to)\n",
    "print(\"目录列表：\", os.listdir(extract_to))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69bd2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['古巴爬虫1', '古巴爬虫2', '古巴爬虫3', '古巴爬虫4', '古巴爬虫5']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(r\"E:\\股吧爬虫\\股吧爬虫\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38900d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 配置文件夹路径\n",
    "folder_path = r'E:\\股吧爬虫\\股吧爬虫\\古巴爬虫1\\切分'\n",
    "\n",
    "# 2. 准备一个列表，用来记录每个文件是否为空\n",
    "empty_files = []\n",
    "nonempty_files = []\n",
    "\n",
    "# 3. 遍历文件夹中的所有 .xlsx 和 .xls 文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    # 3.1 跳过临时文件\n",
    "    if filename.startswith('~$'):\n",
    "        continue\n",
    "    # 3.2 只处理 Excel 文件\n",
    "    if not (filename.endswith('.xlsx') or filename.endswith('.xls')):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    try:\n",
    "        # 4. 读取所有 sheet 到字典\n",
    "        sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "    except Exception as e:\n",
    "        print(f\"无法读取 {filename}：{e}\")\n",
    "        continue\n",
    "\n",
    "    # 5. 检查是否所有 sheet 都为空\n",
    "    all_empty = True\n",
    "    for sheet_name, df in sheets.items():\n",
    "        if not df.empty:\n",
    "            all_empty = False\n",
    "            break\n",
    "\n",
    "    if all_empty:\n",
    "        # 6. 如果全空，记录并删除该文件\n",
    "        empty_files.append(filename)\n",
    "        os.remove(file_path)\n",
    "        print(f\"删除空文件：{filename}\")\n",
    "    else:\n",
    "        nonempty_files.append(filename)\n",
    "\n",
    "# 7. 将非空文件列表保存到新的 Excel（可选）\n",
    "backup_df = pd.DataFrame({'NonEmptyFiles': nonempty_files})\n",
    "backup_path = os.path.join(folder_path, '非空文件列表.xlsx')\n",
    "backup_df.to_excel(backup_path, index=False)\n",
    "print(f\"已将 {len(nonempty_files)} 个非空文件名单保存至：{backup_path}\")\n",
    "\n",
    "# 8. 打印删除和保留情况\n",
    "print(f\"共删除 {len(empty_files)} 个空文件：\", empty_files)\n",
    "print(f\"共保留 {len(nonempty_files)} 个文件，列表已保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "695832fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保留待检查的文件数：5321\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder = r'E:\\股吧爬虫\\股吧爬虫\\古巴爬虫1\\切分'\n",
    "size_threshold = 10 * 1024  # 10 KB\n",
    "\n",
    "candidates = []\n",
    "for fn in os.listdir(folder):\n",
    "    if fn.startswith('~$') or not fn.endswith(('.xlsx',' .xls')): \n",
    "        continue\n",
    "    fp = os.path.join(folder, fn)\n",
    "    if os.path.getsize(fp) < size_threshold:\n",
    "        # 小文件，极可能是空表，直接删\n",
    "        os.remove(fp)\n",
    "    else:\n",
    "        candidates.append(fn)\n",
    "\n",
    "print(f\"保留待检查的文件数：{len(candidates)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deaea51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除文件数：0；保留文件数：5321\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "folder = r'E:\\股吧爬虫\\股吧爬虫\\古巴爬虫1\\切分'\n",
    "to_delete = []\n",
    "to_keep = []\n",
    "\n",
    "for fn in os.listdir(folder):\n",
    "    if fn.startswith('~$') or not fn.endswith(('.xlsx','.xls')):\n",
    "        continue\n",
    "    fp = os.path.join(folder, fn)\n",
    "    try:\n",
    "        # 只读模式打开，不加载所有单元格到内存\n",
    "        wb = load_workbook(fp, read_only=True, data_only=True)\n",
    "    except Exception as e:\n",
    "        print(f\"无法打开 {fn}，跳过。\")\n",
    "        to_delete.append(fn)\n",
    "        continue\n",
    "\n",
    "    # 检查所有 sheet，只要有一个 sheet 行数>1 就保留\n",
    "    nonempty = False\n",
    "    for sheet in wb.worksheets:\n",
    "        if sheet.max_row > 1:  \n",
    "            nonempty = True\n",
    "            break\n",
    "    wb.close()\n",
    "\n",
    "    if nonempty:\n",
    "        to_keep.append(fn)\n",
    "    else:\n",
    "        to_delete.append(fn)\n",
    "        os.remove(fp)\n",
    "\n",
    "print(f\"删除文件数：{len(to_delete)}；保留文件数：{len(to_keep)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f27383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有文件已合并至 CSV： E:\\股吧爬虫\\股吧爬虫\\全部公司评论.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r'E:\\股吧爬虫\\股吧爬虫\\古巴爬虫1\\切分'\n",
    "output_csv = r'E:\\股吧爬虫\\股吧爬虫\\全部公司评论.csv'\n",
    "\n",
    "# 找到所有 .xlsx 文件\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
    "\n",
    "# 1. 用第一份文件确定列名并写入 CSV（写入 header）\n",
    "first_df = pd.read_excel(os.path.join(folder_path, files[0]), dtype=str)\n",
    "first_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 2. 其余文件逐个读取并追加写入（不带 header）\n",
    "for f in files[1:]:\n",
    "    path = os.path.join(folder_path, f)\n",
    "    df = pd.read_excel(path, dtype=str)\n",
    "    df.to_csv(output_csv, mode='a', index=False, header=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"所有文件已合并至 CSV：\", output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = r'E:\\股吧爬虫\\股吧爬虫\\全部公司评论.csv'\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    data_path,\n",
    "    skiprows=1,              # 跳过第一行\n",
    "    header=0,                # 第二行作为列名\n",
    "    dtype=str,\n",
    "    parse_dates=['时间'],     # 指定解析“时间”列\n",
    "    encoding='utf-8-sig',\n",
    "    engine='python',         # 更宽松的 Python 引擎\n",
    "    on_bad_lines='skip',     # 跳过格式不对的行\n",
    "    chunksize=100_000        # 每次读取10万行\n",
    "):\n",
    "    print(\"读取块，行数：\", len(chunk))\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# 合并所有块\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "print(\"总读取行数：\", len(df))\n",
    "print(\"列名：\", df.columns.tolist())\n",
    "\n",
    "\n",
    "# 4. 统计每个 Symbol 的评论数\n",
    "counts = df.groupby('Symbol').size().reset_index(name='comment_count')\n",
    "print(counts['comment_count'].describe())\n",
    "\n",
    "# 5. 剔除评论数太少的公司\n",
    "threshold = 30\n",
    "removed = counts[counts['comment_count'] < threshold]\n",
    "print(f\"将剔除 {len(removed)} 家评论数 < {threshold} 的股票\")\n",
    "\n",
    "valid = counts[counts['comment_count'] >= threshold]['Symbol']\n",
    "df = df[df['Symbol'].isin(valid)].reset_index(drop=True)\n",
    "print(\"剔除后剩余总评论条数：\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1d2733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论总数分布：\n",
      "count      5322.000000\n",
      "mean      12158.739947\n",
      "std       15101.904716\n",
      "min         320.000000\n",
      "25%        3760.000000\n",
      "50%        7759.000000\n",
      "75%       15119.750000\n",
      "max      251568.000000\n",
      "Name: total_comments, dtype: float64\n",
      "将剔除 0 家评论 < 30 的公司\n",
      "保留 5322 家评论 ≥ 30 的公司\n",
      "统计报告已保存至： E:\\inform\\comment_count_report.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "data_path = r'E:\\股吧爬虫\\股吧爬虫\\全部公司评论.csv'\n",
    "CHUNKSIZE = 200_000  # 块大小，根据内存情况可调整\n",
    "\n",
    "comment_counter = Counter()\n",
    "\n",
    "# 分块读取，只读取“股票代码”列\n",
    "for chunk in pd.read_csv(\n",
    "    data_path,\n",
    "    skiprows=1,\n",
    "    header=0,\n",
    "    usecols=['股票代码'],   # 只提取这一列\n",
    "    dtype=str,\n",
    "    encoding='utf-8-sig',\n",
    "    engine='python',\n",
    "    on_bad_lines='skip',\n",
    "    chunksize=CHUNKSIZE\n",
    "):\n",
    "    # 直接累计计数\n",
    "    comment_counter.update(chunk['股票代码'])\n",
    "\n",
    "# 构建统计表\n",
    "df_counts = pd.DataFrame({\n",
    "    'Symbol': list(comment_counter.keys()),\n",
    "    'total_comments': list(comment_counter.values())\n",
    "})\n",
    "\n",
    "# 描述统计\n",
    "desc = df_counts['total_comments'].describe()\n",
    "print(\"评论总数分布：\")\n",
    "print(desc)\n",
    "\n",
    "# 剔除阈值\n",
    "threshold = 30\n",
    "removed = df_counts[df_counts['total_comments'] < threshold]\n",
    "kept = df_counts[df_counts['total_comments'] >= threshold]\n",
    "print(f\"将剔除 {len(removed)} 家评论 < {threshold} 的公司\")\n",
    "print(f\"保留 {len(kept)} 家评论 ≥ {threshold} 的公司\")\n",
    "\n",
    "# 保存统计结果\n",
    "report_path = r'E:\\inform\\comment_count_report.csv'\n",
    "df_counts.to_csv(report_path, index=False, encoding='utf-8-sig')\n",
    "print(\"统计报告已保存至：\", report_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38841263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取到列 `4` 共 79921 条原始记录\n",
      "—— 单支股票 000001 评论情感分析 ——\n",
      "总评论数：79912\n",
      "负面评论数：54161\n",
      "负面占比：67.78%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "# ——— 1. 配置 ———\n",
    "file_path = r'E:\\股吧爬虫\\股吧爬虫\\古巴爬虫1\\切分\\000001.xlsx'\n",
    "NEG_THRESHOLD = 0.5\n",
    "\n",
    "# ——— 2. 读取 Excel ———\n",
    "# 假设你的评论正文在第 5 列（Python 索引 4），如果是“标题”列，改用 df['标题']\n",
    "df = pd.read_excel(file_path, dtype=str)\n",
    "text_col = df.columns[4]  \n",
    "print(f\"读取到列 `{text_col}` 共 {len(df)} 条原始记录\")\n",
    "\n",
    "# ——— 3. 安全情感打分函数 ———\n",
    "def safe_sentiment(text: str) -> float:\n",
    "    t = (text or '').strip()\n",
    "    if not t:\n",
    "        return 0.5\n",
    "    try:\n",
    "        return SnowNLP(t).sentiments\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "# ——— 4. 对这一只股票的所有评论打分 ———\n",
    "# 去掉空值\n",
    "texts = df[text_col].dropna().astype(str).tolist()\n",
    "scores = [safe_sentiment(t) for t in texts]\n",
    "\n",
    "# ——— 5. 统计负面占比 ———\n",
    "total = len(scores)\n",
    "neg_count = sum(1 for s in scores if s < NEG_THRESHOLD)\n",
    "neg_ratio = neg_count / total if total else 0\n",
    "\n",
    "print(f\"—— 单支股票 000001 评论情感分析 ——\")\n",
    "print(f\"总评论数：{total}\")\n",
    "print(f\"负面评论数：{neg_count}\")\n",
    "print(f\"负面占比：{neg_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67820c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取到列 `4` 共 82561 条原始记录\n",
      "—— 单支股票 000002 评论情感分析 ——\n",
      "总评论数：82553\n",
      "负面评论数：41442\n",
      "负面占比：50.20%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "# ——— 1. 配置 ———\n",
    "file_path = r'E:\\股吧爬虫\\股吧爬虫\\古巴爬虫1\\切分\\000002.xlsx'\n",
    "NEG_THRESHOLD = 0.5\n",
    "\n",
    "# ——— 2. 读取 Excel ———\n",
    "# 假设你的评论正文在第 5 列（Python 索引 4），如果是“标题”列，改用 df['标题']\n",
    "df = pd.read_excel(file_path, dtype=str)\n",
    "text_col = df.columns[4]  \n",
    "print(f\"读取到列 `{text_col}` 共 {len(df)} 条原始记录\")\n",
    "\n",
    "# ——— 3. 安全情感打分函数 ———\n",
    "def safe_sentiment(text: str) -> float:\n",
    "    t = (text or '').strip()\n",
    "    if not t:\n",
    "        return 0.5\n",
    "    try:\n",
    "        return SnowNLP(t).sentiments\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "# ——— 4. 对这一只股票的所有评论打分 ———\n",
    "# 去掉空值\n",
    "texts = df[text_col].dropna().astype(str).tolist()\n",
    "scores = [safe_sentiment(t) for t in texts]\n",
    "\n",
    "# ——— 5. 统计负面占比 ———\n",
    "total = len(scores)\n",
    "neg_count = sum(1 for s in scores if s < NEG_THRESHOLD)\n",
    "neg_ratio = neg_count / total if total else 0\n",
    "\n",
    "print(f\"—— 单支股票 000002 评论情感分析 ——\")\n",
    "print(f\"总评论数：{total}\")\n",
    "print(f\"负面评论数：{neg_count}\")\n",
    "print(f\"负面占比：{neg_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53be7ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取到列 `4` 共 20161 条原始记录\n",
      "—— 单支股票 000004 评论情感分析 ——\n",
      "总评论数：20160\n",
      "负面评论数：11928\n",
      "负面占比：59.17%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "# ——— 1. 配置 ———\n",
    "file_path = r'E:\\股吧爬虫\\股吧爬虫\\古巴爬虫1\\切分\\000004.xlsx'\n",
    "NEG_THRESHOLD = 0.5\n",
    "\n",
    "# ——— 2. 读取 Excel ———\n",
    "# 假设你的评论正文在第 5 列（Python 索引 4），如果是“标题”列，改用 df['标题']\n",
    "df = pd.read_excel(file_path, dtype=str)\n",
    "text_col = df.columns[4]  \n",
    "print(f\"读取到列 `{text_col}` 共 {len(df)} 条原始记录\")\n",
    "\n",
    "# ——— 3. 安全情感打分函数 ———\n",
    "def safe_sentiment(text: str) -> float:\n",
    "    t = (text or '').strip()\n",
    "    if not t:\n",
    "        return 0.5\n",
    "    try:\n",
    "        return SnowNLP(t).sentiments\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "# ——— 4. 对这一只股票的所有评论打分 ———\n",
    "# 去掉空值\n",
    "texts = df[text_col].dropna().astype(str).tolist()\n",
    "scores = [safe_sentiment(t) for t in texts]\n",
    "\n",
    "# ——— 5. 统计负面占比 ———\n",
    "total = len(scores)\n",
    "neg_count = sum(1 for s in scores if s < NEG_THRESHOLD)\n",
    "neg_ratio = neg_count / total if total else 0\n",
    "\n",
    "print(f\"—— 单支股票 000004 评论情感分析 ——\")\n",
    "print(f\"总评论数：{total}\")\n",
    "print(f\"负面评论数：{neg_count}\")\n",
    "print(f\"负面占比：{neg_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301c40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分目录下共 5321 个文件，它们是：\n",
      "['000001.xlsx', '000002.xlsx', '000004.xlsx', '000005.xlsx', '000006.xlsx', '000007.xlsx', '000008.xlsx', '000009.xlsx', '000010.xlsx', '000011.xlsx', '000012.xlsx', '000014.xlsx', '000016.xlsx', '000017.xlsx', '000019.xlsx', '000020.xlsx', '000021.xlsx', '000023.xlsx', '000025.xlsx', '000026.xlsx', '000027.xlsx', '000028.xlsx', '000029.xlsx', '000030.xlsx', '000031.xlsx', '000032.xlsx', '000034.xlsx', '000035.xlsx', '000036.xlsx', '000037.xlsx', '000039.xlsx', '000040.xlsx', '000042.xlsx', '000045.xlsx', '000046.xlsx', '000048.xlsx', '000049.xlsx', '000050.xlsx', '000055.xlsx', '000056.xlsx', '000058.xlsx', '000059.xlsx', '000060.xlsx', '000061.xlsx', '000062.xlsx', '000063.xlsx', '000065.xlsx', '000066.xlsx', '000068.xlsx', '000069.xlsx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = r'E:\\股吧爬虫\\股吧爬虫\\古巴爬虫1\\切分'\n",
    "all_files = os.listdir(base_dir)\n",
    "print(f\"切分目录下共 {len(all_files)} 个文件，它们是：\")\n",
    "print(all_files[:50])  # 只展示前 50 个，确认文件名格式\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
